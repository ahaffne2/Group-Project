---
title: "Group 8 Project"
authors: "Alex Haffner, Erica Winters & Chris Hargis"
output: pdf_document
---
```{r}

df = read.csv("C:/Users/charg/Desktop/Masters/Churn_Train.csv")  ##importing dataset

```

See the ratio of the target variable "churn". We can see here that we have enough variability of the target variable

```{r}
table(df$churn)
```

Check for NAs
```{r}
summary(df)
```

Removing the NA values from the number_vmail_messages column

```{r}
library(tidyr)

df = df[!is.na(df$number_vmail_messages),]
df = df[!is.na(df$account_length),]
df = df[!is.na(df$total_eve_minutes),]
df = df[!is.na(df$total_intl_calls),]

table(df$churn)

```


Using random forest to see the importance of the variables. The higher the value the more important. 

```{r}
#install.packages("randomForest")
library(randomForest)

df$churn <- as.factor(df$churn)

output.forest1 = randomForest(churn ~ account_length + number_vmail_messages + total_day_minutes + total_day_calls + total_day_charge +
                                total_eve_minutes + total_eve_calls + total_eve_charge + total_night_minutes + total_night_calls +
                                total_night_charge + total_intl_minutes + total_intl_calls + total_intl_charge + number_customer_service_calls
                              + international_plan + voice_mail_plan + state + area_code, data = df)
randomForest::importance(output.forest1)


```
```{r}
varImpPlot(output.forest1)
```

We can see from the chart the importance of the variables. We will use these in our model. 
Using all the variables in the model can result to overfitting. 



Create training and test sample sets:

Create Training Set:
```{r}
no_churn_df = df[which(df$churn == "no"), ]
yes_churn_df = df[which(df$churn == "yes"), ]

```

Create training set to 75% of data
```{r}
set.seed(120)
training_data_1 = sample(1:nrow(no_churn_df), 0.01*nrow(no_churn_df))
training_data_2 = sample(1:nrow(yes_churn_df), 0.01*nrow(yes_churn_df))

training_1 = no_churn_df[training_data_1, ]
training_2 = yes_churn_df[training_data_2, ]

training_data = rbind(training_1, training_2)

```


Creating the Test Set
```{r}
test_1 = no_churn_df[-training_data_1, ]        #the opposite of what the training data was
test_2 = yes_churn_df[-training_data_2, ]

test_data = rbind(test_1, test_2)

```


Building the Model

```{r}
model = glm(churn ~ total_day_charge + number_customer_service_calls + total_day_minutes + international_plan + total_eve_charge,
            data = training_data, family = binomial(link = "logit"))
```

Using the test data for predictions
```{r}
predicted = predict(model, test_data)

test_data$Predicted = predicted    #adding predictions to data

```

Evaluating the model
```{r}
model_summary = summary(model)
model_summary$coefficients
```


```{r}
AIC(model)   #Akaike's Information Criteria
BIC(model)   #Bayesian Information Criteria. We want these to be as low as possible. (Under 100)
            
```



```{r}
anova(model, test = "Chisq")
```


```{r}
summary(model)
```
